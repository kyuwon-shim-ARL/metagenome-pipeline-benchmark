# Streamlit ëŒ€ì‹œë³´ë“œ ì„¤ê³„ ë¬¸ì„œ

## 1. ê°œìš”

ë©”íƒ€ê²Œë†ˆ íŒŒì´í”„ë¼ì¸ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ê´€ë¦¬í•˜ëŠ” ì›¹ ê¸°ë°˜ ëŒ€ì‹œë³´ë“œ

## 2. ê¸°ìˆ  ìŠ¤íƒ

- **Frontend**: Streamlit 1.30+
- **Backend**: FastAPI (API ì„œë²„)
- **Database**: SQLite (ë¡œì»¬) / PostgreSQL (í”„ë¡œë•ì…˜)
- **Visualization**: Plotly, Altair, Matplotlib
- **Deployment**: Docker + Nginx

## 3. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
dashboard/
â”œâ”€â”€ app.py                    # ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜
â”œâ”€â”€ pages/                    # ë©€í‹°í˜ì´ì§€ ì•±
â”‚   â”œâ”€â”€ 1_ğŸƒ_Run_Benchmark.py
â”‚   â”œâ”€â”€ 2_ğŸ“Š_Compare.py
â”‚   â”œâ”€â”€ 3_ğŸŒ_Community.py
â”‚   â”œâ”€â”€ 4_ğŸ“ˆ_Analytics.py
â”‚   â””â”€â”€ 5_âš™ï¸_Settings.py
â”œâ”€â”€ components/               # ì¬ì‚¬ìš© ì»´í¬ë„ŒíŠ¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics_card.py
â”‚   â”œâ”€â”€ pipeline_selector.py
â”‚   â”œâ”€â”€ result_viewer.py
â”‚   â””â”€â”€ comparison_chart.py
â”œâ”€â”€ utils/                    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ chart_builder.py
â”‚   â””â”€â”€ api_client.py
â”œâ”€â”€ static/                   # ì •ì  íŒŒì¼
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ images/
â”œâ”€â”€ config/                   # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ dashboard.yaml
â”‚   â””â”€â”€ themes.py
â””â”€â”€ requirements.txt          # ì˜ì¡´ì„±

```

## 4. í˜ì´ì§€ë³„ ì„¤ê³„

### 4.1 í™ˆí˜ì´ì§€ (app.py)

```python
# dashboard/app.py
import streamlit as st
import pandas as pd
from utils.data_loader import load_benchmark_summary
from components.metrics_card import MetricsCard

st.set_page_config(
    page_title="Metagenome Pipeline Benchmark",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

def main():
    st.title("ğŸ§¬ Metagenome Pipeline Benchmark Dashboard")
    
    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.image("static/images/logo.png", width=200)
        st.markdown("---")
        
        # í”„ë¡œì íŠ¸ ì„ íƒ
        projects = load_available_projects()
        selected_project = st.selectbox(
            "Select Project",
            projects,
            index=0
        )
        
        # í•„í„° ì˜µì…˜
        st.subheader("Filters")
        date_range = st.date_input(
            "Date Range",
            value=(datetime.now() - timedelta(days=30), datetime.now())
        )
        
    # ë©”ì¸ ëŒ€ì‹œë³´ë“œ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        MetricsCard(
            title="Total Benchmarks",
            value="127",
            delta="+12 this week",
            color="blue"
        )
    
    with col2:
        MetricsCard(
            title="Active Projects",
            value="8",
            delta="+2 new",
            color="green"
        )
    
    with col3:
        MetricsCard(
            title="Best Pipeline",
            value="nf-core/mag",
            subtitle="93% accuracy",
            color="purple"
        )
    
    with col4:
        MetricsCard(
            title="Avg Runtime",
            value="18.5h",
            delta="-2.3h improved",
            color="orange"
        )
    
    # ìµœê·¼ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    st.subheader("ğŸ“Š Recent Benchmark Results")
    
    recent_results = load_recent_results(selected_project)
    
    # ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸
    fig = create_performance_chart(recent_results)
    st.plotly_chart(fig, use_container_width=True)
    
    # ìƒì„¸ í…Œì´ë¸”
    with st.expander("View Detailed Results"):
        st.dataframe(
            recent_results,
            use_container_width=True,
            hide_index=True
        )

if __name__ == "__main__":
    main()
```

### 4.2 ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ í˜ì´ì§€

```python
# dashboard/pages/1_ğŸƒ_Run_Benchmark.py
import streamlit as st
from utils.api_client import BenchmarkAPI
from components.pipeline_selector import PipelineSelector

st.title("ğŸƒ Run New Benchmark")

# Step 1: ë°ì´í„° ì…ë ¥
st.header("1. Select Input Data")

data_source = st.radio(
    "Data Source",
    ["Upload Files", "Use Test Dataset", "From URL"]
)

if data_source == "Upload Files":
    uploaded_files = st.file_uploader(
        "Choose FASTQ files",
        type=['fastq', 'fq', 'gz'],
        accept_multiple_files=True
    )
    
    if uploaded_files:
        st.success(f"âœ… {len(uploaded_files)} files uploaded")

elif data_source == "Use Test Dataset":
    test_datasets = ["CAMI2", "MetaSUB Sample", "Mock Community"]
    selected_dataset = st.selectbox("Select Dataset", test_datasets)

# Step 2: íŒŒì´í”„ë¼ì¸ ì„ íƒ
st.header("2. Select Pipelines")

pipelines = PipelineSelector()
selected_pipelines = pipelines.render()

# Step 3: íŒŒë¼ë¯¸í„° ì„¤ì •
st.header("3. Configure Parameters")

with st.expander("Advanced Settings", expanded=False):
    col1, col2 = st.columns(2)
    
    with col1:
        min_contig_size = st.number_input(
            "Min Contig Size",
            min_value=500,
            max_value=10000,
            value=1500,
            step=500
        )
        
        max_memory = st.slider(
            "Max Memory (GB)",
            min_value=8,
            max_value=500,
            value=100,
            step=8
        )
    
    with col2:
        num_threads = st.number_input(
            "Number of Threads",
            min_value=1,
            max_value=64,
            value=16
        )
        
        assemblers = st.multiselect(
            "Assemblers",
            ["MEGAHIT", "SPAdes", "metaSPAdes"],
            default=["MEGAHIT"]
        )

# Step 4: ì‹¤í–‰
st.header("4. Run Benchmark")

col1, col2, col3 = st.columns([1, 1, 2])

with col1:
    run_button = st.button(
        "ğŸš€ Start Benchmark",
        type="primary",
        use_container_width=True
    )

with col2:
    dry_run = st.button(
        "ğŸ” Dry Run",
        use_container_width=True
    )

if run_button:
    with st.spinner("Starting benchmark..."):
        api = BenchmarkAPI()
        run_id = api.start_benchmark(
            data=uploaded_files or selected_dataset,
            pipelines=selected_pipelines,
            params={
                'min_contig_size': min_contig_size,
                'max_memory': max_memory,
                'threads': num_threads,
                'assemblers': assemblers
            }
        )
    
    st.success(f"âœ… Benchmark started! Run ID: {run_id}")
    
    # ì‹¤ì‹œê°„ ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_placeholder = st.empty()
    status_placeholder = st.empty()
    
    while True:
        status = api.get_status(run_id)
        
        progress_placeholder.progress(
            status['progress'],
            text=f"Progress: {status['progress']}%"
        )
        
        status_placeholder.info(
            f"Current step: {status['current_step']}"
        )
        
        if status['completed']:
            break
        
        time.sleep(5)
    
    st.balloons()
    st.success("ğŸ‰ Benchmark completed!")
    
    # ê²°ê³¼ ìš”ì•½ í‘œì‹œ
    results = api.get_results(run_id)
    st.subheader("Results Summary")
    st.json(results['summary'])
```

### 4.3 ë¹„êµ ë¶„ì„ í˜ì´ì§€

```python
# dashboard/pages/2_ğŸ“Š_Compare.py
import streamlit as st
import plotly.graph_objects as go
from utils.chart_builder import create_comparison_charts

st.title("ğŸ“Š Pipeline Comparison")

# ë¹„êµí•  ë²¤ì¹˜ë§ˆí¬ ì„ íƒ
st.header("Select Benchmarks to Compare")

col1, col2 = st.columns(2)

with col1:
    run_a = st.selectbox(
        "Benchmark A",
        get_available_runs(),
        format_func=lambda x: f"{x['date']} - {x['pipeline']}"
    )

with col2:
    run_b = st.selectbox(
        "Benchmark B",
        get_available_runs(),
        format_func=lambda x: f"{x['date']} - {x['pipeline']}"
    )

# ë¹„êµ ë©”íŠ¸ë¦­ ì„ íƒ
metrics_to_compare = st.multiselect(
    "Select Metrics",
    ["Completeness", "Contamination", "N50", "Runtime", "Memory", "F1-Score"],
    default=["Completeness", "Runtime", "F1-Score"]
)

if st.button("Compare", type="primary"):
    # ë°ì´í„° ë¡œë“œ
    data_a = load_benchmark_data(run_a)
    data_b = load_benchmark_data(run_b)
    
    # ë¹„êµ ì°¨íŠ¸ ìƒì„±
    st.subheader("ğŸ“ˆ Comparison Results")
    
    # ë ˆì´ë” ì°¨íŠ¸
    fig_radar = create_radar_chart(data_a, data_b, metrics_to_compare)
    st.plotly_chart(fig_radar, use_container_width=True)
    
    # ìƒì„¸ ë¹„êµ í…Œì´ë¸”
    comparison_df = create_comparison_table(data_a, data_b)
    
    st.subheader("ğŸ“‹ Detailed Comparison")
    st.dataframe(
        comparison_df.style.highlight_max(axis=0, color='lightgreen'),
        use_container_width=True
    )
    
    # ìŠ¹ì íŒì •
    winner = determine_winner(data_a, data_b, metrics_to_compare)
    
    if winner:
        st.success(f"ğŸ† **{winner['name']}** performs better overall!")
        st.metric(
            "Performance Advantage",
            f"{winner['advantage']:.1%}",
            delta=f"+{winner['delta']:.1f} points"
        )
```

### 4.4 ì»¤ë®¤ë‹ˆí‹° í˜ì´ì§€

```python
# dashboard/pages/3_ğŸŒ_Community.py
import streamlit as st
import pandas as pd
from utils.api_client import CommunityAPI

st.title("ğŸŒ Community Benchmarks")

# ë¦¬ë”ë³´ë“œ
st.header("ğŸ† Leaderboard")

category = st.selectbox(
    "Category",
    ["Overall", "AMR Detection", "Taxonomic Classification", "Assembly Quality"]
)

timeframe = st.radio(
    "Timeframe",
    ["All Time", "This Month", "This Week"],
    horizontal=True
)

# ë¦¬ë”ë³´ë“œ í‘œì‹œ
leaderboard = load_leaderboard(category, timeframe)

for idx, entry in enumerate(leaderboard[:10]):
    col1, col2, col3, col4 = st.columns([1, 3, 2, 2])
    
    with col1:
        if idx < 3:
            st.markdown(f"### {['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰'][idx]}")
        else:
            st.markdown(f"### #{idx+1}")
    
    with col2:
        st.markdown(f"**{entry['project_name']}**")
        st.caption(f"by {entry['team']}")
    
    with col3:
        st.metric("Score", f"{entry['score']:.2f}")
    
    with col4:
        st.button(
            "View Details",
            key=f"view_{entry['id']}",
            use_container_width=True
        )

# ì»¤ë®¤ë‹ˆí‹° í†µê³„
st.header("ğŸ“Š Community Statistics")

col1, col2, col3 = st.columns(3)

with col1:
    st.metric("Total Benchmarks", "1,247")
    st.metric("Active Projects", "89")

with col2:
    st.metric("Avg Completeness", "87.3%")
    st.metric("Avg Runtime", "22.4h")

with col3:
    st.metric("Best F1-Score", "0.94")
    st.metric("Data Processed", "127 TB")

# íŠ¸ë Œë“œ ì°¨íŠ¸
st.header("ğŸ“ˆ Trends")

trend_metric = st.selectbox(
    "Select Metric",
    ["Performance", "Usage", "Accuracy"]
)

fig_trend = create_trend_chart(trend_metric)
st.plotly_chart(fig_trend, use_container_width=True)
```

### 4.5 ì„¤ì • í˜ì´ì§€

```python
# dashboard/pages/5_âš™ï¸_Settings.py
import streamlit as st
from utils.config_manager import ConfigManager

st.title("âš™ï¸ Settings")

tabs = st.tabs(["General", "Database", "API", "Appearance"])

with tabs[0]:  # General
    st.header("General Settings")
    
    project_name = st.text_input(
        "Project Name",
        value=st.session_state.get('project_name', '')
    )
    
    auto_upload = st.checkbox(
        "Auto-upload results to community",
        value=False
    )
    
    notification_email = st.text_input(
        "Notification Email",
        placeholder="user@example.com"
    )

with tabs[1]:  # Database
    st.header("Database Configuration")
    
    st.info("ğŸ”— Current DB Root: `/data/shared/metagenome-db`")
    
    # DB ìƒíƒœ ì²´í¬
    db_status = check_database_status()
    
    for db_name, status in db_status.items():
        col1, col2, col3 = st.columns([2, 1, 1])
        
        with col1:
            st.text(db_name)
        
        with col2:
            if status['available']:
                st.success("âœ… Available")
            else:
                st.error("âŒ Missing")
        
        with col3:
            st.caption(status['size'])
    
    if st.button("Setup Database Links"):
        setup_database_links()
        st.rerun()

with tabs[2]:  # API
    st.header("API Configuration")
    
    api_url = st.text_input(
        "Central Hub URL",
        value="http://localhost:8000"
    )
    
    api_key = st.text_input(
        "API Key",
        type="password",
        value=st.session_state.get('api_key', '')
    )
    
    if st.button("Test Connection"):
        if test_api_connection(api_url, api_key):
            st.success("âœ… Connection successful!")
        else:
            st.error("âŒ Connection failed")

with tabs[3]:  # Appearance
    st.header("Appearance")
    
    theme = st.selectbox(
        "Theme",
        ["Light", "Dark", "Auto"]
    )
    
    chart_style = st.selectbox(
        "Chart Style",
        ["Plotly", "Altair", "Matplotlib"]
    )
    
    show_tooltips = st.checkbox(
        "Show tooltips",
        value=True
    )

# ì €ì¥ ë²„íŠ¼
if st.button("ğŸ’¾ Save Settings", type="primary"):
    save_settings({
        'project_name': project_name,
        'auto_upload': auto_upload,
        'notification_email': notification_email,
        'api_url': api_url,
        'api_key': api_key,
        'theme': theme,
        'chart_style': chart_style,
        'show_tooltips': show_tooltips
    })
    st.success("âœ… Settings saved successfully!")
```

## 5. ì»´í¬ë„ŒíŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬

### 5.1 ë©”íŠ¸ë¦­ ì¹´ë“œ ì»´í¬ë„ŒíŠ¸

```python
# dashboard/components/metrics_card.py
import streamlit as st

def MetricsCard(title, value, delta=None, subtitle=None, color="blue"):
    """ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ë©”íŠ¸ë¦­ ì¹´ë“œ ì»´í¬ë„ŒíŠ¸"""
    
    colors = {
        "blue": "#1f77b4",
        "green": "#2ca02c",
        "orange": "#ff7f0e",
        "purple": "#9467bd"
    }
    
    with st.container():
        st.markdown(
            f"""
            <div style="
                background: linear-gradient(135deg, {colors[color]}22 0%, {colors[color]}11 100%);
                border-left: 4px solid {colors[color]};
                padding: 1rem;
                border-radius: 0.5rem;
                margin: 0.5rem 0;
            ">
                <div style="color: #666; font-size: 0.9rem;">{title}</div>
                <div style="font-size: 1.8rem; font-weight: bold; color: {colors[color]};">
                    {value}
                </div>
                {f'<div style="color: #666; font-size: 0.8rem;">{subtitle}</div>' if subtitle else ''}
                {f'<div style="color: {"green" if "+" in str(delta) else "red"}; font-size: 0.9rem;">{delta}</div>' if delta else ''}
            </div>
            """,
            unsafe_allow_html=True
        )
```

## 6. ë°°í¬ êµ¬ì„±

### 6.1 Docker ì„¤ì •

```dockerfile
# dashboard/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# ì˜ì¡´ì„± ì„¤ì¹˜
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ì•± ë³µì‚¬
COPY . .

# Streamlit í¬íŠ¸
EXPOSE 8501

# í—¬ìŠ¤ì²´í¬
HEALTHCHECK CMD curl --fail http://localhost:8501/_stcore/health

# ì‹¤í–‰
CMD ["streamlit", "run", "app.py", \
     "--server.port=8501", \
     "--server.address=0.0.0.0", \
     "--server.headless=true"]
```

### 6.2 docker-compose.yml

```yaml
# docker-compose.yml
version: '3.8'

services:
  dashboard:
    build: ./dashboard
    ports:
      - "8501:8501"
    environment:
      - METAGENOME_DB_ROOT=/data/shared/metagenome-db
      - API_URL=http://api:8000
    volumes:
      - ./dashboard:/app
      - /data/shared/metagenome-db:/data/shared/metagenome-db:ro
      - ./results:/results
    depends_on:
      - api
      - postgres
    
  api:
    build: ./api
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://user:pass@postgres:5432/benchmark
    
  postgres:
    image: postgres:15
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=pass
      - POSTGRES_DB=benchmark
    volumes:
      - postgres_data:/var/lib/postgresql/data

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
    depends_on:
      - dashboard

volumes:
  postgres_data:
```

## 7. ë³´ì•ˆ ì„¤ì •

### 7.1 ì¸ì¦ ì‹œìŠ¤í…œ

```python
# dashboard/utils/auth.py
import streamlit as st
import streamlit_authenticator as stauth

def setup_authentication():
    """Streamlit ì¸ì¦ ì„¤ì •"""
    
    # ì‚¬ìš©ì ì •ë³´ (ì‹¤ì œë¡œëŠ” DBì—ì„œ ë¡œë“œ)
    credentials = {
        "usernames": {
            "admin": {
                "name": "Administrator",
                "password": hash_password("admin123"),
                "email": "admin@example.com"
            },
            "user": {
                "name": "User",
                "password": hash_password("user123"),
                "email": "user@example.com"
            }
        }
    }
    
    authenticator = stauth.Authenticate(
        credentials,
        "benchmark_dashboard",
        "auth_key_123",
        cookie_expiry_days=30
    )
    
    return authenticator

# ì•± ì‹œì‘ ì‹œ ì¸ì¦
authenticator = setup_authentication()
name, authentication_status, username = authenticator.login()

if authentication_status:
    st.write(f'Welcome *{name}*')
    # ë©”ì¸ ì•± ë¡œì§
elif authentication_status == False:
    st.error('Username/password is incorrect')
elif authentication_status == None:
    st.warning('Please enter your username and password')
```

## 8. ëª¨ë‹ˆí„°ë§ ë° ë¡œê¹…

```python
# dashboard/utils/monitoring.py
import logging
from datetime import datetime

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/dashboard.log'),
        logging.StreamHandler()
    ]
)

def log_user_action(username, action, details=None):
    """ì‚¬ìš©ì ì•¡ì…˜ ë¡œê¹…"""
    logger = logging.getLogger('user_actions')
    logger.info(f"User: {username}, Action: {action}, Details: {details}")

def track_performance(func):
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë°ì½”ë ˆì´í„°"""
    def wrapper(*args, **kwargs):
        start = datetime.now()
        result = func(*args, **kwargs)
        duration = (datetime.now() - start).total_seconds()
        
        if duration > 1:  # 1ì´ˆ ì´ìƒ ê±¸ë¦¬ë©´ ê²½ê³ 
            logging.warning(f"{func.__name__} took {duration:.2f}s")
        
        return result
    return wrapper
```

## 9. ì„±ëŠ¥ ìµœì í™”

### 9.1 ìºì‹± ì „ëµ

```python
# dashboard/utils/cache.py
import streamlit as st
from functools import lru_cache

@st.cache_data(ttl=600)  # 10ë¶„ ìºì‹œ
def load_benchmark_data(run_id):
    """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ìºì‹±"""
    return fetch_from_database(run_id)

@st.cache_resource  # ì„¸ì…˜ ë™ì•ˆ ìœ ì§€
def get_database_connection():
    """DB ì—°ê²° ìºì‹±"""
    return create_connection()

@lru_cache(maxsize=128)
def calculate_metrics(data_hash):
    """ê³„ì‚° ê²°ê³¼ ìºì‹±"""
    return expensive_calculation(data_hash)
```

## 10. í…ŒìŠ¤íŠ¸ ë° CI/CD

```yaml
# .github/workflows/dashboard.yml
name: Dashboard CI/CD

on:
  push:
    paths:
      - 'dashboard/**'

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          cd dashboard
          pip install -r requirements.txt
          pip install pytest streamlit-testing
      
      - name: Run tests
        run: |
          cd dashboard
          pytest tests/
      
      - name: Build Docker image
        run: |
          docker build -t metagenome-benchmark-dashboard ./dashboard
      
      - name: Deploy to server
        if: github.ref == 'refs/heads/main'
        run: |
          docker push registry.example.com/metagenome-benchmark-dashboard:latest
```

ì´ ì„¤ê³„ë¡œ ì•ˆì „í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ Streamlit ëŒ€ì‹œë³´ë“œë¥¼ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.